{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1\n",
    "Nosso primeiro projeto avaliativo, consistindo em 15% da nota final no curso, será uma aplicação de aprendizado supervisionado: classificar discursos presidenciais feitos para audiências internacionais. Ao todo, são 350 discursos proferidos por Lula, Dilma e Temer que poderão ser utilizados para teste e treino de modelos. Neste trabalho, vocês poderão aplicar o pré-processamento que julgarem mais adequado.\n",
    "\n",
    "Proposta de Guilherme Tiengo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição de procedimentos\n",
    "\n",
    "Para essa atividade, foram realizados:\n",
    "* Três pré-processamentos de dados diferentes, utilizando lemmatização, stem e limpeza comum de dados.\n",
    "* Para cada pré-processamento:\n",
    "    * Aplicação de um algoritmo de NaiveBayes.\n",
    "    * Aplicação de um algoritmo de Floresta Aleatória.\n",
    "* Depois do treinamento dos algoritmos, verificamos o precision e selecionamos 2 algoritmos.\n",
    "* Para finalizar, serão aplicados os algoritmos a base de teste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré processamento\n",
    "O interessante de realizar esse teste é que muitos autores brasileiros utilizam um processo de stem sem especificar qual algoritmo estão utilizando, esse método faz muito sentido para o inglês mas talvez não seja o mais adequado para português.\n",
    "\n",
    "* Lemmatização: Para realizar a lemmatização, foi utilizada a biblioteca 'pt_core_news_md' que apresenta nível de precision de 0.97, documentação disponível em: https://spacy.io/models/pt\n",
    "* Stem: Para o stem, foi selecionada um algoritmo desenvolvido para português, explicação disponível em: https://www.inf.ufrgs.br/~viviane/rslp/index.htm\n",
    "* Básico: os procedimentos estão listados abaixo.\n",
    "\n",
    "Em todos os processamentos foram realizados os seguintes procedimentos:\n",
    "* Retiradas palavras com menos de 4 caracteres,\n",
    "* Removidos os acentos,\n",
    "* Retiradas palavras na lista de stopwords,\n",
    "* Palavras são transformadas em minúsculo.\n",
    "\n",
    "### Normalização\n",
    "\n",
    "Em todos os casos, foi calculado o valor tf-idf que é o produto da frequência do termo no documento (tf) com o inverso da frequência do termo no conjunto de documentos (idf), além de normalizar os termos, essa transformação já é um primeiro ajuste de importância das palavras no corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "\n",
    "Foram utilizados dois modelos que constroem a classificação de formas diferentes, um modelo generativo (Naive Bayes) e um modelo discriminativo (Random Forest). A primeira calcula a probabilidade de determinada classe ser de um grupo ou de outro, os modelos discriminativos acabam tendo uma perfomance melhor e entendem quais features são mais importantes para a classificação.\n",
    "\n",
    "### Perfomance dos modelos\n",
    "\n",
    "![](precision.png)\n",
    "\n",
    "#### Comentário\n",
    "\n",
    "Os modelos que melhor performaram foram os modelos sem a lemmatização e stem.\n",
    "\n",
    "\n",
    "### Melhoria do modelo NaiveBayes\n",
    "\n",
    "Há uma oportunidade de ajuste nos modelos de NaiveBayes através da mudança do alpha (transformação de LaPlace), abaixo é possível ver que o alpha em 0.2 contribui para a melhora do modelo e tem a menor dispersão.\n",
    "\n",
    "![](alphas.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentários finais\n",
    "\n",
    "Os dois algoritmos selecionados tiveram o mesmo padrão de resposta para todos os textos, como melhoria adicional, poderia ser aplicado um terceiro algoritmo \n",
    "para funcionar como critério de desempate (um terceiro juíz), mas nesse caso não foi necessário.\n",
    "\n",
    "Embora a lemmatização tenha tido um desempenho inferior neste processamento, ela parece ser um procedimento interessante em um corpus que tenham muitos documentos seja pela redução de processamento como também por eventualmente aumentar a precisão do modelo. Outro ponto que chama atenção é que trabalhos em português que apresentem stemmização devem apresentar o algoritmo já que os algortimos mais comum foram apenas largamente testados para o inglês.\n",
    "\n",
    "Há recursos do spacy como análise sintática e morfológica que poderiam ser utilizadas como itens de refinamento dos modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Além dos textos referênciados na disciplina, para esse trabalho foi utilizado como referência o livro Speech and Language Processing (cap 2, 4 e 6) - https://web.stanford.edu/~jurafsky/slp3/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('basepy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2eb00c3b99515e73a2f87fbbd4229836e43f7521b301a1b867788ab771a420b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

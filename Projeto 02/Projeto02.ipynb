{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 02"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "O objetivo central dessa atividade é treinar um modelo de deep learning que consiga classificar imagens de locais de votação como sendo URBANAS ou RURAIS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "A entrega deverá ser feita na pasta do GitHub de cada um contendo:\n",
    "\n",
    "Um notebook ou script com o código utilizado;\n",
    "\n",
    "Um documento (pode ser um PDF compilado pelo notebook, mas também pode ser um README.md) detalhando a metodologia utilizada:\n",
    "\n",
    "* Arquitetura utilizada\n",
    "* Hiper-parâmetros utilizados\n",
    "* Resumo dos resultados obtidos\n",
    "* Predição para a base de validação com imagens de satélite sem labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar bibliotecas\n",
    "from tensorflow.random import set_seed \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Rescaling\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 63s 958ms/step - loss: 15.6506 - accuracy: 0.7030 - val_loss: 0.3653 - val_accuracy: 0.8460\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 60s 937ms/step - loss: 0.3131 - accuracy: 0.8675 - val_loss: 0.3440 - val_accuracy: 0.8600\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 57s 892ms/step - loss: 0.1692 - accuracy: 0.9433 - val_loss: 0.4190 - val_accuracy: 0.8140\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 57s 893ms/step - loss: 0.0743 - accuracy: 0.9820 - val_loss: 0.5679 - val_accuracy: 0.7680\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 57s 902ms/step - loss: 0.0324 - accuracy: 0.9942 - val_loss: 0.4663 - val_accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 57s 887ms/step - loss: 0.0302 - accuracy: 0.9948 - val_loss: 0.4826 - val_accuracy: 0.7920\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 59s 930ms/step - loss: 0.0235 - accuracy: 0.9962 - val_loss: 0.4965 - val_accuracy: 0.8020\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 60s 930ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.5007 - val_accuracy: 0.8200\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 58s 897ms/step - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.5234 - val_accuracy: 0.8100\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 57s 893ms/step - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.5256 - val_accuracy: 0.7940\n",
      "8/8 [==============================] - 3s 232ms/step - loss: 0.4211 - accuracy: 0.8280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4211234450340271, 0.828000009059906]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelo 1\n",
    "\n",
    "seed = 44\n",
    "pixel = (350,350)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(350,350,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=10, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 122s 2s/step - loss: 1.5365 - accuracy: 0.7555 - val_loss: 0.3606 - val_accuracy: 0.8540\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 127s 2s/step - loss: 0.4142 - accuracy: 0.7810 - val_loss: 0.4791 - val_accuracy: 0.7520\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 121s 2s/step - loss: 0.2991 - accuracy: 0.8760 - val_loss: 0.3657 - val_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 123s 2s/step - loss: 0.1707 - accuracy: 0.9377 - val_loss: 0.4453 - val_accuracy: 0.8020\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 118s 2s/step - loss: 0.0893 - accuracy: 0.9778 - val_loss: 0.4022 - val_accuracy: 0.8240\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 120s 2s/step - loss: 0.0444 - accuracy: 0.9895 - val_loss: 0.5042 - val_accuracy: 0.7980\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 123s 2s/step - loss: 0.0367 - accuracy: 0.9933 - val_loss: 0.6367 - val_accuracy: 0.7680\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 119s 2s/step - loss: 0.0265 - accuracy: 0.9940 - val_loss: 0.4604 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 134s 2s/step - loss: 0.0489 - accuracy: 0.9837 - val_loss: 0.6237 - val_accuracy: 0.7720\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 122s 2s/step - loss: 0.0146 - accuracy: 0.9973 - val_loss: 0.7202 - val_accuracy: 0.7840\n",
      "8/8 [==============================] - 5s 487ms/step - loss: 0.6428 - accuracy: 0.8140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6428177952766418, 0.8140000104904175]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelo 2\n",
    "\n",
    "seed = 44\n",
    "pixel = (350,350)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(350,350,1)))\n",
    "model.add(Conv2D(12, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=10, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 49s 757ms/step - loss: 0.9690 - accuracy: 0.7720 - val_loss: 0.3797 - val_accuracy: 0.8580\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 50s 775ms/step - loss: 0.3145 - accuracy: 0.8748 - val_loss: 0.3283 - val_accuracy: 0.8660\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 48s 746ms/step - loss: 0.2315 - accuracy: 0.9168 - val_loss: 0.3525 - val_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 48s 743ms/step - loss: 0.1374 - accuracy: 0.9535 - val_loss: 0.4424 - val_accuracy: 0.8180\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 46s 722ms/step - loss: 0.0907 - accuracy: 0.9743 - val_loss: 0.4392 - val_accuracy: 0.8060\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 47s 741ms/step - loss: 0.0613 - accuracy: 0.9818 - val_loss: 0.7139 - val_accuracy: 0.8140\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 55s 858ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.8345 - val_accuracy: 0.8140\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 51s 790ms/step - loss: 0.0105 - accuracy: 0.9977 - val_loss: 0.8229 - val_accuracy: 0.8120\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 49s 765ms/step - loss: 0.0098 - accuracy: 0.9980 - val_loss: 0.9928 - val_accuracy: 0.8040\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 48s 746ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.9812 - val_accuracy: 0.7940\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.8635 - accuracy: 0.8140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8634732961654663, 0.8140000104904175]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelo 3\n",
    "\n",
    "seed = 44\n",
    "pixel = (200,200)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(200,200,1)))\n",
    "model.add(Conv2D(12, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=10, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 124s 2s/step - loss: 0.7301 - accuracy: 0.7172 - val_loss: 0.6678 - val_accuracy: 0.6760\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 121s 2s/step - loss: 0.6427 - accuracy: 0.7247 - val_loss: 0.6441 - val_accuracy: 0.6760\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 122s 2s/step - loss: 0.6136 - accuracy: 0.7247 - val_loss: 0.6319 - val_accuracy: 0.6760\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 123s 2s/step - loss: 0.5970 - accuracy: 0.7247 - val_loss: 0.6300 - val_accuracy: 0.6760\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 119s 2s/step - loss: 0.5906 - accuracy: 0.7247 - val_loss: 0.6322 - val_accuracy: 0.6760\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 125s 2s/step - loss: 0.5889 - accuracy: 0.7247 - val_loss: 0.6340 - val_accuracy: 0.6760\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 124s 2s/step - loss: 0.5886 - accuracy: 0.7247 - val_loss: 0.6348 - val_accuracy: 0.6760\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 121s 2s/step - loss: 0.5885 - accuracy: 0.7247 - val_loss: 0.6351 - val_accuracy: 0.6760\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 125s 2s/step - loss: 0.5885 - accuracy: 0.7247 - val_loss: 0.6354 - val_accuracy: 0.6760\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 125s 2s/step - loss: 0.5885 - accuracy: 0.7247 - val_loss: 0.6354 - val_accuracy: 0.6760\n",
      "8/8 [==============================] - 6s 582ms/step - loss: 0.5409 - accuracy: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5409040451049805, 0.7739999890327454]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelo 4\n",
    "\n",
    "seed = 44\n",
    "pixel = (350,350)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(350,350,1)))\n",
    "model.add(Conv2D(12, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=10, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "63/63 [==============================] - 123s 2s/step - loss: 2.1809 - accuracy: 0.7642 - val_loss: 0.3989 - val_accuracy: 0.8320\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 127s 2s/step - loss: 0.3017 - accuracy: 0.8698 - val_loss: 0.3245 - val_accuracy: 0.8680\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 145s 2s/step - loss: 0.2566 - accuracy: 0.9057 - val_loss: 0.3730 - val_accuracy: 0.8480\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 121s 2s/step - loss: 0.1486 - accuracy: 0.9520 - val_loss: 0.4374 - val_accuracy: 0.7740\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 122s 2s/step - loss: 0.1784 - accuracy: 0.9283 - val_loss: 0.3813 - val_accuracy: 0.8360\n",
      "8/8 [==============================] - 5s 453ms/step - loss: 0.3219 - accuracy: 0.8480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3219097852706909, 0.8479999899864197]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelo 5 \n",
    "\n",
    "seed = 44\n",
    "pixel = (350,350)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(350,350,1)))\n",
    "model.add(Conv2D(12, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=5, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelo5/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"modelo5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Há algum conflito entre o pandas e o Keras no servidor que estou utilizando\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "modelo5 = load_model(\"modelo5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 44\n",
    "pixel = (350,350)\n",
    "batch_size  = 64\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validacao = []\n",
    "for x,y in validacao.as_numpy_iterator():\n",
    "    y_validacao.append(y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 20s 2s/step\n"
     ]
    }
   ],
   "source": [
    "resultados = modelo5.predict(validacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.concatenate(y_validacao),columns=[validacao.class_names[1]])\n",
    "df2 = pd.DataFrame(np.round(resultados[:,1],0),columns=[\"predito_urbano\"])\n",
    "pd.concat([df1,df2],axis=1).to_excel(\"resultados.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,  89],\n",
       "       [ 67, 320]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(np.concatenate(y_validacao), np.round(resultados[:,1],0),labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rural'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 44\n",
    "pixel = (200,200)\n",
    "batch_size  = 64\n",
    "\n",
    "treino = image_dataset_from_directory(\n",
    "    'mapas/treino',    \n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "teste = image_dataset_from_directory(\n",
    "    'mapas/teste',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "validacao = image_dataset_from_directory(\n",
    "    'mapas/validacao',\n",
    "    label_mode='categorical',\n",
    "    seed=seed,  image_size=pixel,  batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "# Define a arquitetura do modelo\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(200,200,1)))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treina\n",
    "model.fit(treino, batch_size=64, epochs=5, validation_data=(teste))\n",
    "\n",
    "# Avalia\n",
    "model.evaluate(validacao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4a4f96f49ea232b26132451f8a7f70ff484841ea2acd500542a64290b9d59e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
